\documentclass[conference]{IEEEtran}


%\usepackage{geometry}
\usepackage{balance}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{mdwmath}
\usepackage{mdwtab}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{pifont}
\usepackage{algpseudocode}
\usepackage{balance}
\usepackage{bm}
\usepackage{ulem}
\usepackage{array}
\usepackage{balance}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{threeparttable}
\frenchspacing
\usepackage[marginal]{footmisc}
\usepackage{extarrows}
\usepackage{natbib}

%add the command initial
\algnewcommand\algorithmicInitial{\textbf{Initialize:}}
\algnewcommand\Initial{\item[\algorithmicInitial]}
\algnewcommand\algorithmicIterate{\textbf{Iterate:}}
\algnewcommand\Iterate{\item[\algorithmicIterate]}



\ifCLASSINFOpdf

\else

\fi



% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
%\title{Bare Demo of IEEEtran.cls\\ for IEEE Conferences}
\title{SVRG with Adaptive Epoch Size}

% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{Erxue Min,
Yawei~Zhao,
Jun Long,
Chengkun Wu,
Kuan Li,
Jianping Yin

\author{\IEEEauthorblockN{Erxue Min, Yawei Zhao, Jun Long, Chengkun Wu, Kuan Li, and Jianping Yin}
\IEEEauthorblockA{College of Computer, National University of Defense Technology,\\
Changsha, China 410073
%Email: {myw123, wchknudtli.kuan}@163.com, {zhaoyawei,jpyin}@nudt.edu.cn.
}
}
\IEEEcompsocitemizethanks
{\IEEEcompsocthanksitem E. Min,Y. Zhao, J. Long, C. Wu, K. Li and J. Yin are with College of Computer, National University of Defense Technology, Changsha 410073, P.R. China. 
}
}

%\author{\IEEEauthorblockN{Erxue Min}
%\IEEEauthorblockA{National University of \\Defense Technology\\
%}
%\and
%\IEEEauthorblockN{Yawei Zhao}
%\IEEEauthorblockA{National University of \\Defense Technology\\
%}
%\and
%\IEEEauthorblockN{XXXXX}
%\IEEEauthorblockA{National University of \\Defense Technology\\
%}
%}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3}, 
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}


\balance

% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}

% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
    Stochastic gradient descent (SGD) is widely used for large-scale machine learning tasks, but converges slowly due to the highly inherent variance. In recent years, the popular Stochastic Variance Reduced Gradient (SVRG) method mitigates this shortcoming, through computing the full gradient of the entire dataset for every epoch.  However, conventional SVRG and its variants usually need  to identify this hyper-parameter, i.e. the epoch size, which is essential to the convergence performance. Few previous studies discuss the method to identify such the hyper-parameter, which makes it hard to gain a good convergence performance in practical machine learning tasks.  In this paper, we propose a new stochastic gradient descent with the variance reduction technique named \textsc{aeSVRG} which computes the full gradient adaptively.  Moreover, we propose an improved method denoted by \textsc{aeSVRG+}, whose convergence performance is comparable to and even better than that of SVRG with best-tuned epoch sizes. Extensive empirical studies illustrate the significant advantage of our proposed algorithms.


\end{abstract}

% no keywords

% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
% no \IEEEPARstart
Many machine learning tasks such as neural networks \citep{Dean2012Large}, logistic regression \citep{Roux2012A} and matrix factorization \citep{Sa2015Global} can be formulated to be an optimization problem which is described as 
\begin{equation}
\label{equation_loss_minimization}
\min F(\omega),~~~~~F(\omega)=\frac{1}{n}\sum\limits_{i=1}^n f_i(\omega)+R(\omega)
\end{equation}
where $n$ is the size of the training data. $F(\omega)$ means the loss function or training loss of a machine learning model, and $\omega$ is its parameter. $R(\omega)$ is the regularizer, which is widely used to avoid overfitting. It is noting that the total number of instances in the training dataset, i.e. $n$ becomes very large with the proliferation of data. 

Gradient Descent (GD) \citep{Boyd2013Convex} is a basic method to solve such the optimization problem. The gradient of $F(\omega)$ denoted by $\nabla F(\omega)$  is obtained by passing over the entire training data, which is extremely time-consuming when the size of training data, i.e. $n$ becomes large.  Besides, GD is an iterative-convergent algorithm, that is, the parameter, i.e. $\omega$, usually needs thousands of iterations to be converged.   Since GD needs to compute the  gradient of  $F(\omega)$ every iteration, when the volume of data is large, the computation cost increases sharply and impairs the convergent performance significantly. 

Stochastic Gradient Descent (SGD) \citep{Robbins1951A} mitigates this shortcoming by replacing the calculation of $\nabla F(\omega)$ with a stochastic gradient denoted by $\nabla f_i(\omega)$ with $i\in\{1,2, ..., n\}$. In SGD, $i$ is selected randomly  from the entire training data. Thus, SGD outperforms GD on the time efficiency significantly. Take the expectation of $i$, we obtain $\mathbb{E}[\nabla f_i(\omega)] = \nabla F(\omega)$. The difference between $\nabla f_i(\omega)$ and  $\nabla F(\omega)$ represents \emph{variance} which makes it difficult to achieve the optimum.  In order to make the loss function, i.e. $F(\omega)$ converge, a decaying learning rate is usually used to reduce the variance. However, value of the learning rate is decayed to be very small after hundreds of iterations, which impedes the loss function to converge. In a nutshell, SGD with a decaying learning rate incurs a sub-linear convergence rate.

%In recent years, variance reduced variants of SGD such as SAG \citep{Schmidt:2013ui}, SAGA \citep{Defazio:2014vu}, SDCA \citep{ShalevShwartz:2016vy} are proposed to reduce the variance and gain the linear convergence performance.         

% for strongly convex problems but requires to store all the component gradients, which bring in an intolerable memory burden. 
In recent years, variance reduced variants of SGD such as SVRG \citep{Johnson:9MAvkbgy} are proposed to reduce the variance. Those methods gain the linear convergence performance with a constant learning rate. In SVRG, a full gradient is computed occasionally during the inexpensive SGD steps, and thus divides the optimization procedure into many epochs. SVRG uses such the full gradient to reduce the variance during the train of parameters. On the basis of SVRG, many variants have been proposed to improve its performance.
SVRG-BB \citep{Tan2016Barzilai} uses the Barzilai and Borwein (BB) method   to compute the step size before every epoch \citep{Barzilai1988Two}. 
\textsc{CheapSVRG} \citep{Shah2016Trading}  aims at reducing the expensive computational cost of full gradient  through using a surrogate with a subset of the training dataset. 
mS2GD \citep{Liu:2015bx} uses mini-batch method to obtain a full gradient to reduce the variance, which shows a clear advantage for  parallel computation.  EMGD \citep{Zhang2013Linear}, SVR-GHT \citep{Li:2016vh}, Prox-SVRG \citep{Xiao:2014vw} and SVRG with second-order information \citep{KolteAccelerating} modify the update rule of stochastic steps, and show advantages to SVRG in some cases.  However,  most previous researches present that the epoch size, i.e. $m$ should be constant \citep{Johnson:9MAvkbgy, Tan2016Barzilai, Shah2016Trading} or increased monotonically \citep{Liu:2015bx},  regardless of the learning rate. It is recommended that $m = 2n$ for convex problems and $m = 5n$ for non-convex problems in SVRG, without theoretical analysis and further experimental verification. Extensive empirical studies illustrate that the choice of good value for those hyper-parameters costs much time in real machine learning tasks.  

Generally, the epoch size, i.e. $m$ has a great impact on the convergence performance of SVRG.  More specifically, when $m$ is too small, it wastes too much time to compute the full gradient frequently. When $m$ is rather large, the variance between the stochastic gradient and the full gradient increases sharply, making the convergence of training loss extremely difficult. According to the analysis of variance \cite{AllenZhu:2016up}, both the epoch size, i.e. $m$ and learning rate, i.e. $\eta$ have a significant impact on the convergence performance.  However, those previous studies do not provide a practical method to set the value of those hyper-parameters. 

Since SVRG applies SGD with variance reducer to optimize the object function, the training loss decreases rapidly at the beginning of each epoch but tends to fluctuate after excessive iterations. If we can detect the fluctuation and break the current epoch as soon as it happens, we are sure to achieve a better performance than the original SVRG. Thus the key idea is to find an effective strategy to detect the fluctuation in real time. In this paper, we develop a novel algorithm denoted by \textsc{aeSVRG} which can adjust the epoch size adaptively.  \textsc{aeSVRG} applies a new stop condition regarding the variance and the change rate of parameters. Besides, such the stop condition terminates the iterations in an epoch by considering the value of learning rate.  Additionally, we give guidance of how to set the parameters in practical machine learning tasks. Since \textsc{aeSVRG} may stop iterations earlier than expectation when $\eta$ is quite small, we propose an improved algorithm denoted by \textsc{aeSVRG+}. In a nutshell, our contributions are highlighted as follows:
\begin {itemize}
\item  A novel algorithm denoted by \textsc{aeSVRG} is proposed, which can adjust the epoch size to a suitable value dynamically.
\item An improvement of \textsc{aeSVRG} denoted by \textsc{aeSVRG+}  is   presented, which  is not sensitive to the selection of hyper-parameters, e.g. the learning rate and the epoch size.
\item Extensive empirical studies shows the effectiveness of our proposed algorithms which outperform their counterparts on the convergence performance significantly.
\end {itemize}

This paper is organized as follows: Section \ref{sectiove_related_work} reviews the related work. Section \ref{svrg_review} describes the SVRG algorithm. Section \ref{mywork} presents the new variant of SVRG, i.e. \textsc{aeSVRG} and its improved method, i.e. \textsc{aeSVRG+}. Section \ref{numexperiments} demonstrates the numerical results of our algorithms. Section \ref{discussion} discusses the strengths and weaknesses.
Section \ref{conclusion} concludes this paper.



\section{Related work}
\label{sectiove_related_work}
Several variants of SVRG  discussing the strategy of adjusting epoch size has been proposed, including SVRG++ \citep{Allen2015Improved}, S2GD \citep{Richtarik:2013te}, SVRG\_Auto\_Epoch \citep{Allen2015Improved} and so on. 

SVRG++ adopts a simple strategy that epoch size, i.e. $m$ doubles between every consecutive two epochs. This method is absolutely heuristic and sometimes not justified. Our experiments show that when $\eta$ is large or moderate, the exponential growth of $m$ will incur great variance and impairs convergence. 

S2GD designs a probability model of $m$ and shows that a large epoch size is used with a high probability. However, it needs to know the lower bound of the value of strong convexity of $F(w)$ in Equation \ref{equation_loss_minimization}, which is hard to estimate in practice. Meanwhile, the maximum of stochastic steps per epoch is also a sensitive parameter. 

SVRG\_Auto\_Epoch is introduced as an additional improvement of SVRG++. It determines the termination of epoch through the quality of the snapshot full gradient. It records $diff_t = \Vert\nabla f_{i}(\omega_t^s)\mathrm{-}\nabla f_{i}(\tilde{\omega}^{s-1})\Vert$ every iteration $t$ and uses it as a tight upper bound on the variance of the gradient estimator. Although this method is reasonable, it has too much parameters to tune. Moreover, it takes much additional computation for every iterations, which impairs performances significantly. 
Comparing with the above methods, \textsc{aeSVRG} can adaptively adjusting the epoch size without tuning extra hyper-parameters. Moreover, it takes little additional computation cost and outperforms those previous methods.

%\subsection{Subsection Heading Here}
%Subsection text here.
%\subsubsection{Subsubsection Heading Here}
%Subsubsection text here.

 \begin{algorithm}[t]
 	\caption{\textsc{SVRG}}
	\label{SVRG}
	\begin{algorithmic}[1]
	\Require the learning rate $\eta$,  the epoch size $m$, and an initial point $\tilde{\omega}_0$.
	\For {$s=0, 1, 2, ... $}
		\State $\tilde{\mu} = \frac{1}{n}\sum\limits_{i=1}^{n}\nabla f_{i}(\tilde{\omega}_{s})$
		\State $\omega_0 = \tilde{\omega}_s$
		%compute the full gradient 
		\For {$t=1, 2, ..., m$}
			\State Randomly pick $i_t\in\{1, 2, ..., n\}$
			\State $\omega_t = \omega_{t-1} - \eta(\nabla f_{i_t}(\omega_{t-1}) - \nabla f_{i_t}(\tilde{\omega}_s)+\tilde{\mu})$
		\EndFor
		\State \textbf{Option \uppercase\expandafter{\romannumeral1}:} $\tilde{\omega}_{s+1} = \omega_{m}$
		\State \textbf{Option \uppercase\expandafter{\romannumeral2}:} $\tilde{\omega}_{s+1} = \omega_{t}$ for randomly chosen $t \in \{0, ... ,m - 1\}$ 
	\EndFor
	\end{algorithmic}
\end{algorithm}


\section{Preliminaries}
\label{svrg_review}
 In this section we review the SVRG algorithm proposed by Johnson and Zhang\citep{Johnson:9MAvkbgy}.  As is shown in Algorithm \ref{SVRG}, there are two loops in SVRG. Each outer loop is called an $epoch$ while each inner loop is called an $iteration$. In each outer loop, a full gradient $\tilde{\mu}$ is computed at first, and its calculation requires to scan the entire dataset. In each inner loop,  every iteration needs to pick $i_t\in\{1, 2, ..., n\}$ randomly. The update rule of the parameters is illustrated in Equation \ref{generalized_SGD}:
\begin{equation}
\label{generalized_SGD}
\omega_t = \omega_{t-1} - \eta(\nabla f_{i_t}(\omega_{t-1}) - \nabla f_{i_t}(\tilde{\omega}_s)+\tilde{\mu}).
\end{equation}
Note that the expectation of $\nabla f_{i_t}(\tilde{\omega}_s)$ over $i_t$ is $\tilde{\mu}$, and the expectation of $\nabla f_{i_t}(\omega_{t-1})$ over $i_t$ is $\nabla F(\omega_{t-1})$. We thus obtain
\begin{equation}
\mathbb{E}[\omega_{t} | \omega_{t-1}] = \omega_{t-1} - \eta\nabla F(\omega_{t-1})
\end{equation}
 It can be seen that the variance of the update rule, i.e. Equation \ref{generalized_SGD} is reduced. When both $\tilde{\omega}$ and $\omega_t$ converge to the optimum $\omega^*$, then $\tilde{\mu}\rightarrow0$ and $\nabla f_{i_t}(\omega_{t-1})\rightarrow\nabla f_{i_t}(\tilde{\omega}_s)$, therefore
 $$\nabla f_{i_t}(\omega_{t-1}) - \nabla f_{i_t}(\tilde{\omega}_s)+\tilde{\mu}\rightarrow0$$
 Hence, the learning rate for SVRG is allowed to be set as a relatively large constant against SGD, which results in a high convergence rate.
At the end of each epoch, $\tilde\omega_{s+1}$ is updated by the output of inner loop. Note that there are two options for the update. Although only the convergence of SVRG with Option \uppercase\expandafter{\romannumeral1} is analyzed in \citep{Johnson:9MAvkbgy}, SVRG with Option \uppercase\expandafter{\romannumeral2} has been confirmed numerically to perform better. We adopt Option \uppercase\expandafter{\romannumeral2} in this paper. It is obvious that when $m$ is too large, SGD with variance reducer will degenerate to the basic SGD, which results in huge variance. Thus our work focuses on how to set an appropriate epoch size.
 
 \section{SVRG with Adaptive Epoch Size}
 \label{mywork}
 In this section we describe two novel algorithms: \textsc{aeSVRG} and \textsc{aeSVRG+}, which can set the epoch size adaptively and have significant convergence performance superior to the previous studies.
 We assume the loss function $F(\omega)$ and the component functions $f_i(\omega)$ in Equation \ref{equation_loss_minimization} are convex and smooth throughout the paper.
 
 \subsection{\textsc{aeSVRG}}
 \subsubsection{Idea}
Generally, the optimal $m$ is strongly related to $\eta$. Our experimental results also report that when $\eta$ is large, the training loss begins to fluctuate after merely a small number of iterations. On the other hands, SVRG can endure far more than $n$ iterations with a small $\eta$. In specific, if we stop the iterations in one epoch just before the training loss begins to fluctuate, the new algorithm will certainly be very efficient and outperform SVRG with constant epoch size. A direct approach is to evaluate the training loss occasionally. However, the training loss computation requires to pass over the entire training dataset, which is rather time consuming. 

 When we apply gradient descent to convex problem, as the $\omega_t$ gradually approaches to the optimal value $\omega^*$, the gradient $\nabla F(\omega_t)$ thus keeps decreasing. We know that
 $$\omega_t - \omega_{t-1} = -\eta\nabla F(\omega_{t-1})$$
 Then we have $\Vert\omega_{t+1}-\omega_t\Vert<\Vert\omega_{t}-\omega_{t-1}\Vert$. 
 %It is also apparent that $\Vert\omega_{t+m_0}-\omega_t\Vert<\Vert\omega_{t}-\omega_{t-m_0}\Vert$, where $m_0$ is a positive integer. 
Consider the update rule of SVRG, and take the expectation of $i_t$, we can obtain
 \begin{equation}
\label{sgdinequality}
\begin{split}
\mathbb{E}[\omega_t -\omega_{t-1}] &=  \mathbb{E}[- \eta(\nabla f_{i_t}(\omega_{t-1}) - \nabla f_{i_t}(\tilde{\omega}_s)+\tilde{\mu})]\\
&=-\eta\mathbb{E}[\nabla f_{i_t}(\omega_{t-1}) - \nabla f_{i_t}(\tilde{\omega}_s)+\tilde{\mu}]\\
&=-\eta\nabla F(\omega_{t-1})
\end{split}
\end{equation}
 Intuitively, if $\omega_t$ keeps approaching to the optimal value $\omega^*$, the $\nabla F(\omega_{t})$ decays generally, with slight fluctuation caused by variance. Therefore, if we consider a window, we can believe that $\Vert\omega_{t+m_0}-\omega_t\Vert<\Vert\omega_{t}-\omega_{t-m_0}\Vert$ holds  during the train of parameters.  
 On the other hand, when $\omega_t$ oscillates near the optimal value due to the variance, $\Vert\omega_{t+m_0}-\omega_t\Vert$ will keep fluctuating. 

 Inspired by this observation, \textsc{aeSVRG}  sets 
 \begin{equation}
 \label{stop_condition}
 \Vert\omega_{t+m_0}-\omega_t\Vert>\Vert\omega_{t}-\omega_{t-m_0}\Vert
 \end{equation}
 as a stoping condition in each epoch. If the Inequality \ref{stop_condition} holds, we deem that the training loss fails to converge and begins to fluctuate, we have to suspend the iterations in such epoch and compute a full gradient in order to reduce the variance.
 
  \begin{algorithm}[t]
 	\caption{\textsc{aeSVRG}}
	\label{aeSVRG}
	\begin{algorithmic}[1]
	\Require the learning rate $\eta$, the window size $m_0$, and an initial point $\tilde{\omega}$
	\For {$s=0, 1, ...$}
		\State $\tilde{\mu} = \frac{1}{n}\sum\limits_{i=1}^{n}\nabla f_{i}(\tilde{\omega}_{s})$
		\State $\omega_0 = \tilde{\omega}_s$
		%compute the full gradient 
		\For {$t=1, 2,..$}
			\If{$t\%m_0=0$ and $t>=2m_0$ and $\Vert \omega_t - \omega_{t-m_0}\Vert>\Vert \omega_{t-m_0} - \omega_{t-2m_0}\Vert$}
			\State break
			\EndIf
			\State Randomly pick $i_t\in\{1, 2, ..., n\}$
			\State $\omega_t = \omega_{t-1} - \eta(\nabla f_{i_t}(\omega_{t-1}) - \nabla f_{i_t}(\tilde{\omega}_s)+\tilde{\mu})$
		\EndFor		
		\State $\tilde{\omega}_{s+1} = \omega_{t}$
	\EndFor
	\State \Return $\tilde{\omega}_{s+1}$
	\end{algorithmic}
\end{algorithm}

 
\subsubsection{Details}
As illustrated in Algorithm \ref{aeSVRG}, \textsc{aeSVRG} just requires two parameters: learning rate $\eta$ and window size $m_0$. It differs from SVRG only in the inner loop. At line 5, the first condition, i.e. $t\%m_0=0$ means that  we check Inequality \ref{stop_condition} every $m_0$ iterations. The second condition, i.e. $t>=2m_0$ is trivial as we need at least two windows to check the stoping conditions.  If the condition holds, we break the inner loop, and step out of the current epoch, and into the next epoch. Note that the epoch size is definitely a multiple of $m_0$. Hence we should set $m_0$ to be smaller than $0.5n$ for flexibility. At the same time, $m_0$ cannot be too small because the stop condition may usually hold, which is caused by the random pick of the instances. We recommend to set $m_0$ between $0.1n$ and $0.2n$ according to extensive empirical sutdies.

\subsubsection{Advantages}
\begin{itemize}
\item \textsc{aeSVRG} is superior to SVRG with prefixed epoch size as it can adapt the epoch size to an appropriate value dynamically. For a large $\eta$, \textsc{aeSVRG} will adjust the epoch size to be relatively small to constrain variance. On the other hand, when $\eta$ is small, each epoch can thus contain more iterations. \textsc{aeSVRG} will make the epoch size larger than $2n$ to avoid to compute the time-consuming full gradient frequently.
\item Comparing with the variants of SVRG such as SVRG\_Auto\_Epoch and S2GD, \textsc{aeSVRG} requires far less additional computation cost. The reason is that it just needs to compute a simple inequality every $m_0$ iterations, which is very efficient.
\end{itemize}
 
 \subsection{Optimal Choice of the window size}
 \label{m0_analysis}
 In \textsc{aeSVRG}, we use $\Vert\omega_{t}-\omega_{t-m_0}\Vert$ to detect when the training loss begins to fluctuate and fails to converge. However, how to choose a suitable value of the window size i.e. $m_0$ becomes an important issue. In this section, we analyze the convergence performance by varying settings of $m_0$, and provide guidance on setting an optimal value.
 
Since the variance incurred by SGD iterations cannot be ignored, when we set $m_0$ to be small, the variance of $\Vert\omega_{t}-\omega_{t-m_0}\Vert$ is too large, and  the probability of the stop condition of \textsc{aeSVRG} which is denoted by $C(m_0)$, is relatively high. As a result, the Inequality \ref{stop_condition} may often holds due to variance, even though the loss function is still decreased during iterations. Therefore, the full gradient will be computed frequently, which thus wastes much time.  It is obvious that $C(m_0)$ is a monotonically increasing function of $m_0$.
 
 If we set $m_0$ to be relatively large, it will be too late to detect the oscillation of training loss. Therefore, it wastes much time, and makes no process to optimize the objective function. We use $D_s(m_0)$ to denote the \emph{Absolute Delay} of detecting fluctuation. Furthermore, the \emph{Absolute Delay} makes different effects on the convergence performance, which  depends on the epoch size. Thus, it is better to consider the \emph{Relative Delay}:
 \begin{equation}
 \label{relative_delay}
 D_r(m_0)=\frac{D_s(m_0)}{\upsilon+n}.
 \end{equation}
 Note that $\upsilon$ denote the number of iterations in a specific epoch and $n$ denotes the size of the training dataset. Function $D_r(m_0)$ is also monotonically increasing with respect to $m_0$.

It is necessary to choice a suitable $m_0$  which should ensure that $C(m_0)$ is large and $D_r(m_0)$ is small. In order to obtain a trade-off between $C(m_0)$ and $D_r(m_0)$, we convert the parameter-choosing problem to the following maximization problem:
\begin{equation}
\label{minform0}
\begin{split}
m_0 &= \max\limits_{m_0} \{C(m_0)-D_r(m_0)\}\\
&= \max\limits_{m_0} \{C(m_0)-\frac{D_s(m_0)}{\upsilon+n} \}
\end{split}
\end{equation}
\textrm{s.t.} 
\begin{equation}
0<m_0<n
\end{equation}

From equation (\ref{minform0}) we can obtain the following conclusions:
\begin{enumerate}
\item When epoch size $\upsilon$ is small, the $D_s$ tends to be more important than $C$. Hence we should set $m_0$ to be relatively small to maximize the objective function. On the contrary, it is recommended to set $m_0$ to be large. In spirit of this, we can set $m_0$ to be proportional to the iteration number of previous epoch.
\item According to our experiment on SVRG, when the learning rate $\eta$ is large, the loss function begins to fluctuate after merely $n/10$ iterations, so we should set the initial $m_0$ to the same order of magnitude as $0.1 n$.
 \end{enumerate}
 
 \subsection{\textsc{aeSVRG+}}
 \subsubsection{Idea}
 Our experiments on \textsc{aeSVRG} show that it performs well for large and moderate $\eta$. However, when $\eta$ is rather small, the best epoch size for SVRG will be larger than $10n$. And \textsc{aeSVRG} may finish the epoch prematurely while the training loss keeps declining. It is because the inequality (\ref{stop_condition}) may holds due to variance, rather than the fluctuation of training loss. According to the analysis in \ref{m0_analysis}, variance of $\Vert\omega_{t+m_0}-\omega_t\Vert$ has more impact on the performance of our algorithm when the best epoch size is large. Intuitively, we should adjust the window size $m_0$ of the next epoch according to the current epoch size, instead of a constant value.
 \subsubsection{Details}
 As illustrated in Algorithm \ref{aeSVRG+}, we recompute $m_0$ after the inner loop in each epoch. $m_0$ is set to be $(\lfloor \upsilon/n \rfloor+1) \times (0.1n)$, where $\upsilon$ denotes the iteration number of current epoch size. It means that $m_0$ is incremented by $0.1n$ when $\upsilon$ exceeds $n,2n,3n$ and so on. In other words, for each epoch, we have an expected epoch size equal to $(10m_0-n)$, if the real epoch size is smaller than the expected one, \textsc{aeSVRG+} will decrease the value of $m_0$ according to the size of current epoch. Otherwise it will set $m_0$ to be larger.
 
 
 \subsubsection{Advantages}
 \begin{itemize}
 \item \textsc{aeSVRG+} outperforms \textsc{aeSVRG} as it will enlarge the window size to reduce variance when necessary, avoiding to stop an epoch early. It shows good performance regardless the learning rate and datasets in  our experiments. 
 \item \textsc{aeSVRG+} is not sensitive to the initialized $m_0$, as it tunes $m_0$ to an appropriate size adaptively. By contrast, the performance of \textsc{aeSVRG} fairly depends on the choice of $m_0$.
 \end{itemize}
 
 

 \begin{algorithm}[t]
 	\caption{\textsc{aeSVRG+}}
	\label{aeSVRG+}
	\begin{algorithmic}[1]
	\Require learning rate $\eta$, window size $m_0$, initial point $\tilde{\omega}$
	%\Initial $m_0=0.1n$
	\For {$s=0, 1, ...$}
		\State $\tilde{\mu} = \frac{1}{n}\sum\limits_{i=1}^{n}\nabla f_{i}(\tilde{\omega}_{s})$
		\State $\omega_0 = \tilde{\omega}_s$
		%compute the full gradient 
		\For {$t=1, 2,..$}
			\If{$t>m_0$ and $t\%m_0=0$ and $\Vert \omega_t - \omega_{t-m_0}\Vert>\Vert \omega_{t-m_0} - \omega_{t-2m_0}\Vert$}
			\State break
			\EndIf
			\State Randomly pick $i_t\in\{1, 2, ..., n\}$
			\State $\omega_t = \omega_{t-1} - \eta(\nabla f_{i_t}(\omega_{t-1}) - \nabla f_{i_t}(\tilde{\omega}_s)+\tilde{\mu})$
		\EndFor
		\State $\upsilon = t$	
		\State $m_0$ = $(\lfloor \upsilon/n \rfloor+1) \times (0.1n)$
		\State $\tilde{\omega}_{s+1} = \omega_{t}$
	\EndFor
	\State \Return $\tilde{\omega}_{s+1}$
	\end{algorithmic}
\end{algorithm}
 
% \begin{figure*}[htb]
%\centering
%\subfigure[ijcnn1 $\eta=0.5$]{\includegraphics[width=0.24\linewidth]{cmijcnn105}\label{cmijcnn105}}
%\subfigure[ijcnn1 $\eta=0.05$]{\includegraphics[width=0.24\linewidth]{cmijcnn1005}\label{cmijcnn1005}}
%\subfigure[ijcnn1 $\eta=0.01$]{\includegraphics[width=0.24\linewidth]{cmijcnn1001}\label{cmijcnn1001}}
%\subfigure[ijcnn1 $\eta=0.001$]{\includegraphics[width=0.24\linewidth]{cmijcnn10001}\label{cmijcnn10001}}
%\label{cmijcnn1}
%\caption{Comparison of \textsc{aeSVRG}, \textsc{aeSVRG+}, SVRG++, S2GD}
%\end{figure*}
 \begin{figure*}[htb]
\centering
\subfigure[w8a]{\includegraphics[width=0.5\columnwidth]{cm_w8a01}\label{cm_w8a01}}
\subfigure[mushrooms]{\includegraphics[width=0.5\columnwidth]{cm_mushrooms01}\label{cm_mushrooms}}
\subfigure[abalone]{\includegraphics[width=0.5\columnwidth]{cm_abalone_scale01}\label{cm_abalone01}}
\subfigure[mg]{\includegraphics[width=0.5\columnwidth]{cm_mg_scale01}\label{cm_mg01}}
\caption{Comparison of \textsc{aeSVRG}, \textsc{aeSVRG+}, SVRG++, S2GD on four datasets, with $\eta=0.1$. }
\label{cm_test}
\end{figure*}





 
 \section{Numerical Experiments}
 \label{numexperiments}
 \subsection{Experimental settings}
 In this section, we conduct extensive experiments to demonstrate the advantages of our proposed algorithms. We evaluate our algorithms on eight training datasets, which are public on the LIBSVM website\footnote{http://www.csie.ntu.edu.tw/$\sim$cjlin/libsvmtools/datasets/}. More specifically,  $l2$-regularized logistic regression on datasets: ijcnn1, a9a, w8a and mushrooms, and $l2$-regularized ridge regression on datasets: YearPredictMSD, cadata, mg, and abalone  are conducted to evaluate our proposed algorithms and the previous work.  The details of those datasets are illustrated in Table \ref{data_information}. Note that the five columns denote the name of the dataset, the size of the dataset, the dimension of features, the model applied, the coefficient of regularizer respectively. Besides, \emph{logistic} denotes logistic regression and \emph{ridge} denotes ridge regression. 
 
 
 \begin{table}
\centering
\caption{Details of datasets and models}
\label{data_information}
\begin{tabular}{|l|l|l|l|l|}
\hline
Dataset           & Size & Dimension & Model & $\lambda$ \\ \hline
ijcnn1            &  49990 &  22 &   logistic    &  $10^{-4}$         \\
a9a               &   32561&123   &     logistic  &      $10^{-4}$     \\ 
w8a               &  49749 & 300   &  logistic &  $10^{-4}$     \\ 
mushrooms   &   8124     &112    & logistic   &  $10^{-4}$     \\ 
YearPredictionMSD & 463715  &  90 &    ridge  &      $10^{-4}$     \\
cadata              & 20640  &8   &     ridge  &    $10^{-4}$       \\ 
abalone          & 4177  & 8 &   ridge&   $10^{-4}$       \\ 
mg             &1385 &6 & ridge&  $10^{-4}$     \\ \hline
\end{tabular}
\end{table}
 

 The $l2$-regularized logistic regression task is conducted on datasets: ijcnn1, a9a, w8a and mushrooms where the label of instances  is set to be 1 or -1. Thus, the loss function of $l2$-regularized logistic regression task is formulated:
\begin{equation}
\label{logistic_reg}
\min\limits_\omega \frac{1}{n}\sum\limits_{i=1}^n \log(1+e^{-y_i \omega^\mathrm{T} x_i }) + \lambda \parallel \omega \parallel^2.
\end{equation} Here, $x_i$ is the instances in the training dataset, and $y_i$ is the label of $x_i$. $\lambda$ is the coefficient of regularizer. Additionally, the $l2$-regularized ridge regression task is conducted on  datasets: YearPredictMSD, cadata, mg and abalone. The loss function of $l2$-regularized ridge regression task is formulated:
\begin{equation}
\label{ridge_reg}
\min\limits_\omega \frac{1}{n}\sum\limits_{i=1}^n\left(\omega^{\mathrm{T}}x_i-y_i\right)^2 + \lambda \parallel \omega \parallel^2.
\end{equation}

We scale the value of all features to $[-1,1]$ and set  $\lambda$ to be $10^{-4}$ for all evaluations. 
In all figures, the $x$-axis denotes the computational cost, which is measured by the number of gradient computation divided by the size of training data, i.e. $n$. The $y$-axis denotes training loss residual, i.e. $F(\tilde{\omega}_s) - F(\omega^{*})$. Note that the optimum $F(\omega^{*})$ is estimated by running the gradient descent for a long time. The x-axis in all figures means that the number of gradient calculations divided by the size of training data, i.e. $\mathrm{grad/n}$. This is a metric for measuring gradient complexity in previous work, which is a similar to measure the time cost during train of parameters. Our numerical experiments include three parts: comparison of convergence performance with previous methods, comparison of convergence performance with SVRG by varying learning rate and sensitivity test varying $m_0$. The experimental results report the superior performance of our methods. 

 \begin{figure*}[ht]
\centering
\subfigure[ijcnn1 $\eta=0.5$]{\includegraphics[width=0.5\columnwidth]{ijcnn105}\label{ijcnn105}}
\subfigure[ijcnn1 $\eta=0.05$]{\includegraphics[width=0.5\columnwidth]{ijcnn1005}\label{ijcnn1005}}
\subfigure[ijcnn1 $\eta=0.01$]{\includegraphics[width=0.5\columnwidth]{ijcnn1001}\label{ijcnn1001}}
\subfigure[ijcnn1 $\eta=0.001$]{\includegraphics[width=0.5\columnwidth]{ijcnn10001}\label{ijcnn10001}}

\caption{Generally, \textsc{aeSVRG} can automatically set an appropriate $m$ with different learning rates for the $l2$-regularized logistic regression on the dataset ijcnn1}
\label{figure_logistic_ijcnn1}
\end{figure*}

\begin{figure*}[ht]
\centering
\subfigure[a9a $\eta=0.3$]{\includegraphics[width=0.5\columnwidth]{a9a03}\label{a9a03}}
\subfigure[a9a $\eta=0.05$]{\includegraphics[width=0.5\columnwidth]{a9a005}\label{a9a005}}
\subfigure[a9a $\eta=0.02$]{\includegraphics[width=0.5\columnwidth]{a9a002}\label{a9a002}}
\subfigure[a9a $\eta=0.001$]{\includegraphics[width=0.5\columnwidth]{a9a0001}\label{a9a0001}}

\caption{Generally, \textsc{aeSVRG} can automatically set an appropriate $m$ with different learning rates for the $l2$-regularized logistic regression on the dataset a9a}
\label{figure_logistic_a9a}
\end{figure*}

\begin{figure*}[ht]
\subfigure[YearPredictionMSD $\eta=0.01$]{\includegraphics[width=0.5\columnwidth]{Year_scale001}\label{Year_scale001}}
\subfigure[YearPredictionMSD $\eta=0.005$]{\includegraphics[width=0.5\columnwidth]{Year_scale0005}\label{Year_scale0005}}
\subfigure[YearPredictionMSD $\eta=0.002$]{\includegraphics[width=0.5\columnwidth]{Year_scale0002}\label{Year_scale0002}}
\subfigure[YearPredictionMSD $\eta=0.0001$]{\includegraphics[width=0.5\columnwidth]{Year_scale00001}\label{Year_scale00001}}
\caption{Generally, \textsc{aeSVRG} can automatically set an appropriate $m$ with different learning rates for the $l2$-regularized ridge regression on the dataset YearPredictionMSD}
\label{figure_ridge_Year}
\end{figure*}


\begin{figure*}[ht]
\subfigure[cadata $\eta=0.1$]{\includegraphics[width=0.5\columnwidth]{cadata01}\label{cadata01}}
\subfigure[cadata $\eta=0.05$]{\includegraphics[width=0.5\columnwidth]{cadata005}\label{cadata005}}
\subfigure[cadata $\eta=0.02$]{\includegraphics[width=0.5\columnwidth]{cadata002}\label{cadata002}}
\subfigure[cadata $\eta=0.001$]{\includegraphics[width=0.5\columnwidth]{cadata0001}\label{cadata0001}}
\caption{Generally, \textsc{aeSVRG} can automatically set an appropriate $m$ with different learning rates for the $l2$-regularized ridge regression on the dataset cadata}
\label{figure_ridge_cadata}
\end{figure*}

 \begin{figure*}[ht]
 \label{aeSVRG_aeSVRG+}
\centering
\subfigure[\textsc{aeSVRG} on a9a]{\includegraphics[width=0.5\columnwidth]{m0a9a02}\label{a9a_aeSVRG}}
\subfigure[\textsc{aeSVRG+} on a9a]{\includegraphics[width=0.5\columnwidth]{m0a9a02+}\label{a9a_aeSVRG+}}
\subfigure[\textsc{aeSVRG} on abalone]{\includegraphics[width=0.5\columnwidth]{m0abalone01}\label{abalone_aeSVRG}}
\subfigure[\textsc{aeSVRG+} on abalone]{\includegraphics[width=0.5\columnwidth]{m0abalone01+}\label{abalone_aeSVRG+}}
\caption{Sensitivity test on $m_0$ for \textsc{aeSVRG} and \textsc{aeSVRG+}. The four numbers on the legends means four different choices of $m_0$. The $\eta$ of a9a and abalone are 0.2 and 0.1, respectively.}

\end{figure*}


\subsection{Comparison of convergence performance with previous methods}
In this section, we compare our \textsc{aeSVRG} and \textsc{aeSVRG+} with two aforementioned existing methods: SVRG++ and S2GD. We do not compare with SVRG\_Auto\_Epoch in that we find that its termination condition of epoch is never satisfied and  thus SVRG\_Auto\_Epoch keeps doing SGD iteration, resulting in non-convegent. For SVRG++, we initialize $m = n$. For S2GD, we set the maximum of $m$ to be $4n$. For both \textsc{aeSVRG} and \textsc{aeSVRG+}, we set the window size, i.e. $m_0$ to be $0.1n$. The learning rate, i.e. $\eta$, is fixed as $0.1$ throughout this part of experiments. We evaluate these methods by running logistic regression on the dataset ijcnn1. 
As illustrated in Figure \ref{cm_test}, we can see that SVRG++ always fluctuates violently and fails to converge due the large variance caused by too large epoch size. It is shown that \textsc{aeSVRG} and \textsc{aeSVRG+} always outperform SVRG++ and S2GD and converge rapidly. Besides, the performance of \textsc{aeSVRG+} is superior to that of \textsc{aeSVRG}. The main reason is that \textsc{aeSVRG+} can adjust the windows size to a suitable value adaptively.


 \subsection{Comparison of convergence performance with SVRG by varying learning rate}
 To demonstrate that our algorithm is capable of adjusting epoch size adaptively regarding to the learning rate, we compare our algorithm with SVRG for different $\eta$.
Since \textsc{aeSVRG+} performs better than \textsc{aeSVRG}, only \textsc{aeSVRG+} is used to conduct the comparison with SVRG. For SVRG, we increase the epoch size, i.e. $m$ as four different values: $n$, $2n$, $4n$, $10n$. Those values are used to conduct performance evaluation in SVRG \citep{Johnson:9MAvkbgy}.  Besides, the performance with the epoch size larger than $10n$ is similar  to the performance with an extremely large epoch size. It is because that the full gradient is rare to be computed due to such extremely epoch size. The dashed lines represents SVRG with a fixed epoch size; while the green solid lines stands for \textsc{aeSVRG+}.

As illustrated in Figures \ref{figure_logistic_ijcnn1}, \ref{figure_logistic_a9a}, \ref{figure_ridge_Year}, \ref{figure_ridge_cadata},  \textsc{aeSVRG+} can always have the similar performance as SVRG with most best-tuned epoch size. We observe that when $\eta$ is large,  and $m$ is set to be a small value, e.g. $n$, can achieve best performance. The main reason is that when $\eta$ is large, the variance becomes significant simultaneously, so $m$ must be set to be small in order to bound the variance. As $\eta$ decays, the optimal value of $m$ increases, which means that the algorithm can tolerate more variance induced by extra iterations. As illustrated in Figures \ref{ijcnn105}, \ref{a9a03}, \ref{Year_scale001}, \ref{cadata01}, our method is significantly better than SVRG with best-tuned epoch sizes when learning rate is large or medium. However, As illustrated in Figures \ref{ijcnn10001}, \ref{a9a0001}, \ref{Year_scale00001}, \ref{cadata0001}, if $\eta$ is set to be too small, \textsc{aeSVRG+} performs slightly superior to  SVRG with large epoch sizes, but outperforms SVRG with recommended epoch sizes, i.e. $n$ and $2n$. It is noting that setting $\eta$ to be too small is not a practical approach when using SVRG or its variants, because the convergence rate will be extremely low. Therefore, the sub-optimal performance of \textsc{aeSVRG+} with rather small $\eta$ is acceptable.

 
\subsection{Sensitivity test by varying  $m_0$}
In this section, we conduct the sensitivity test on both \textsc{aeSVRG} and \textsc{aeSVRG+} regarding window size $m_0$. As analyzed in \ref{m0_analysis}, we vary the initial $m_0$ ranges from $0.1$ to $0.25$ in order to test the sensitivity of \textsc{aeSVRG} and \textsc{aeSVRG+}. We conduct the experiments by using logistic regression and ridge regression on two datasets: a9a and abalone, respectively. As illustrated in Figures \ref{a9a_aeSVRG} and \ref{abalone_aeSVRG}, the performance of \textsc{aeSVRG} obviously varies with the $m_0$. And we can see in Figures \ref{a9a_aeSVRG+} and \ref{abalone_aeSVRG+} that the performance of \textsc{aeSVRG+} is not sensitive to the choice of $m_0$. 
The main reason is that \textsc{aeSVRG} uses the prefixed $m_0$ all the time, while \textsc{aeSVRG+} will adjust the $m_0$ adaptively regardless of the initialization.
If the initial value is too large, the stop condition holds just after a few windows of iterations, leading to a smaller epoch size than expected.
Thus \textsc{aeSVRG+} will decrease $m_0$ to a suitable value gradually. On the contrary, \textsc{aeSVRG+} will adjust $m_0$ to bigger according to the size of current epoch.  
Hence \textsc{aeSVRG+} is more practical than \textsc{aeSVRG} in reality.
%It shows that the performance of \textsc{aeSVRG+} is not sensitive to the choice of $m_0$ while the performance of \textsc{aeSVRG} varies with the $m_0$.

\section{Discussion}
\label{discussion}
%?????????????????????????????????????????????????????????????????????
Optimization problems are complex as there are numerous application scenarios, algorithms and datasets. It is impossible for SVRG to always achieve an excellent performance with the same epoch size. Besides, the selection for an optimal epoch size is time-consuming and not feasible for massive data. Several existing methods provide some strategies for how to set the epoch size, but they are not well adapted to the various real problems. Hence our proposed algorithms which can adjust epoch size adaptively are great improvements of SVRG. Although we have not rigorously conducted the convergence analysis of \textsc{aeSVRG} and \textsc{aeSVRG+}, both of them prove to be efficient and have a significant improvement on the original SVRG experimentally. Thus the novel stop condition is a practical strategy for detecting the fluctuation of training loss. We leave it as an open question to prove the rationality theoretically.

\section{Conclusion}
In this paper we propose a novel stop condition for each epoch in SVRG, leading to a new variant of SVRG: \textsc{aeSVRG}, which can adjust the epoch size adaptively. We analyze how to choose the optimal value of parameters, and thus develops an improved method called \textsc{aeSVRG+}. We conduct numerical experiments on real datasets to demonstrate the advantage of convergence performance of \textsc{aeSVRG} and \textsc{aeSVRG+}. The experiments show that both \textsc{aeSVRG} and \textsc{aeSVRG+} are superior to existing methods. Moreover, the \textsc{aeSVRG+} is comparable to and sometimes even better than SVRG with best-tuned epoch size, but does not need to tune its epoch size.
\label{conclusion}


\section*{Acknowledgment}
This work was  supported by the National Natural Science Foundation of China (Project NO. 60970034, 61170287, 61232016, 61303189 and 31501073).





\bibliographystyle{plain}
\bibliography{reference}


% that's all folks
\end{document}


